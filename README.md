# ğŸµ AI-Powered Audio Classification: Real vs Synthetic Detection

![Audio Analysis](https://source.unsplash.com/1600x400/?sound,technology)

## ğŸ“Œ Overview

This repository provides a **high-performance pipeline** for detecting whether an audio file is **real or AI-generated**. The system processes, augments, trains, and classifies audio files using **deep learning** and **spectrogram analysis**, leveraging **PyTorch, torchaudio, librosa, and timm**.

ğŸš€ **Key Features:**
- **ğŸ”„ Audio Augmentation** â€“ Enhance datasets with speed changes, pitch shifts, noise injection, and filtering.
- **ğŸ›ï¸ Format Conversion & Segmentation** â€“ Standardize audio files and split them into fixed-length segments.
- **ğŸ§  Deep Learning Model** â€“ Train and merge multiple classifiers for robust inference.
- **ğŸ“Š Probability-Based Inference** â€“ Process overlapping windows and smooth classification results.

---

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ audio_augmneter_batch.py      # Augments audio data with various transformations
â”œâ”€â”€ audio_convert_segmenter.py    # Splits audio files into 4-sec mono segments
â”œâ”€â”€ audio_convert_wave.py         # Converts audio to WAV format (32kHz, mono, 16-bit PCM)
â”œâ”€â”€ file_rename_hash.py           # Renames files using SHA256 hash values
â”œâ”€â”€ train_submodel.py             # Trains individual binary classifiers on spectrograms
â”œâ”€â”€ merge_model_classifier.py     # Merges multiple sub-models into a unified classifier
â”œâ”€â”€ inference_classifier.py       # Runs inference using the merged classifier model

```

---

## ğŸ“¦ Installation

### âœ… Prerequisites
- **Python 3.10**
- **PyTorch + torchvision + torchaudio**
- **librosa**
- **timm (Torch Image Models)**
- **ffmpeg**

### âš¡ Setup
```bash
# Clone the repository
git clone https://github.com/TtesseractT/Synthetic-Audio-Detection.git
cd Synthetic-Audio-Detection

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ¯ Usage

### ğŸ”„ **Audio Augmentation**
```bash
python audio_augmneter_batch.py -i <input_folder> -o <output_folder>
```

### ğŸ› **Audio Conversion & Segmentation**
```bash
python audio_convert_wave.py -i <input_folder> -o <output_folder>
python audio_convert_segmenter.py -i <input_folder> -o <output_folder>
```

### ğŸ”‘ **File Renaming (SHA256-based)**
```bash
python file_rename_hash.py -i <directory> -r  # -r for recursive mode
```

### ğŸ‹ï¸ **Training a Sub-Model**
```bash
python train_submodel.py --data-dir <dataset_directory> --epochs 100 --batch-size 32
```

### ğŸ”— **Merging Sub-Models into One Classifier**
Prepare a CSV file with columns `model_filename`, `synthetic_class`, and `real_class`:
```bash
python merge_model_classifier.py --submodels-folder <folder> --csv-file <csv_file> --output-path merged_model.pth
```

### ğŸ¯ **Inference on New Audio Files**
```bash
python inference_classifier.py --merged-model merged_model.pth --audio <audio_file.wav> --threshold 0.5 --smooth
```

---

## ğŸ† Model Training & Architecture

ğŸ“Š **Training Workflow:**
1. **Preprocess audio files** â†’ Convert to spectrograms
2. **Train binary classifiers** â†’ Identify real vs synthetic samples
3. **Merge sub-models** â†’ Create a single ensemble classifier
4. **Perform inference** â†’ Classify new audio samples

ğŸ›  **Architecture Highlights:**
- **ResNet-based classifier** (pretrained `timm` models)
- **Binary classification per sub-model** (Real vs Synthetic)
- **Multi-head merged classifier** for robust predictions

---

## ğŸ“ˆ Example Output
ğŸ“Œ Sample classification output in `results.json`:
```json
{
    "filename": "audio_sample.wav",
    "segments": [
        {"start_sec": 0.0, "end_sec": 4.0, "label": "SyntheticOne"},
        {"start_sec": 4.0, "end_sec": 8.0, "label": "Real"}
    ],
    "percentages": {
        "SyntheticOne": 50.24,
        "Real": 48.02
    }
}
```

---

## ğŸ¤ Contributing
We welcome contributions! Please submit a pull request or open an issue for suggestions, bug fixes, or feature requests.

---

## ğŸ“œ License
This project is licensed under the **MIT License**.

---

## ğŸ“š Citations & References
This project leverages open-source frameworks:
- **PyTorch**: https://pytorch.org/
- **torchaudio**: https://pytorch.org/audio/
- **librosa**: https://librosa.org/
- **timm (Torch Image Models)**: https://github.com/rwightman/pytorch-image-models

*Developed with â¤ï¸ by [Your Name / Organization]* ğŸš€

